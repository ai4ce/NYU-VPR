<!DOCTYPE html>
<html>
    <head>
        <link href="//maxcdn.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css" rel="stylesheet" id="bootstrap-css">
        <!-- <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons"> -->
        <script src="https://code.jquery.com/jquery-3.3.1.js"></script>
        <script src="//maxcdn.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/responsive-tabs@1.6.3/js/jquery.responsiveTabs.min.js"></script>
        <!-- <script src='https://kit.fontawesome.com/a076d05399.js'></script> -->


        <title>NYU-VPR Dataset</title>
        <link href="style.css" rel='stylesheet' type='text/css'>
        <script src="scripts.js"></script>
    </head>

    <body>
        <nav>
            <a class="logo" href="https://ai4ce.github.io">
                <img src="imgs/ai4ce_linear.png">
                <!-- <img src="imgs/ai4ce.png"> -->
            </a>
            <div class="menu">
                <ul>
                    <li><a href="#">Home</a></li>
                    <li><a href="#intro">Introduction</a></li>
                    <li><a href="#desp">Description</a></li>
                    <li><a href="#download">Download</a></li>
                    <!-- <li><a href="#exp">Experiences</a></li> -->
                    <li><a href="#contact">Contact</a></li>
                </ul>
            </div>
        </nav>
        <div class="head_container">
            <header class="main_header">
                <div class="box">
                    <div class="title">NYU-VPR Dataset Website</div>
                    <hr class="line">
                    <div class="description">This is the official website for paper <a href="https://arxiv.org/abs/2110.09004">NYU-VPR</a></div>
                </div>
            </header>
        </div>
        
        <section class="main_section" id="intro">
            <div class="content">
                <h1>Introduction</h1>
                <hr class="horizontalLine">
                <p>Our dataset is named NYU-VPR. It is composed of images recorded in Manhattan, New York from April 2016 to March 2017. The images were recorded by cameras installed on the front, back, and side parts of taxis with auto-exposure. The dataset contains both side-view images and front-view images. There are 100,500 side-view and 101,290 front-view images, each with a 640x480 resolution. On the basis of raw images, we use <a href="https://github.com/mseg-dataset/mseg-api" target="_blank" style="text-decoration: none; color: rgb(190, 80, 190);">MSeg</a>, a semantic segmentation method, to replace moving objects such as people and cars with white pixels. Fig. 1 compares anonymized and raw images.</p>

                <div>
                    <img class="imgs" src="imgs/non_anony.jpg" alt="non_anony">
                    <figcaption>Fig. 1 - Illustration of raw images and anonymized images</figcaption>
                </div>

                <p>The images were recorded on streets around Washington Square Park. The trajectories of the locations where the images were recorded are shown in Fig. 2. Since the cameras were placed on fleet cars, and their routes were random, the frequencies of locations where the images were taken are different. The frequencies of the locations where the side-view and front-view images were recorded are shown in Fig. 3 and Fig. 4 respectively.</p>

                <div class="imgs">
                    <div class="one-third">
                        <img src="imgs/all_mark.jpg" alt="trajectories">
                        <figcaption>Fig. 2</figcaption>
                    </div>
                    <div class="one-third">
                        <img src="imgs/freq_front.jpg" alt="freq_front">
                        <figcaption>Fig. 3</figcaption>
                    </div>
                    <div class="one-third">
                        <img src="imgs/freq_side.jpg" alt="freq_side">
                        <figcaption>Fig. 4</figcaption>
                    </div>
                </div>

                <p>Fig. 5 shows the time distribution. Since it contains images captured from May 2016 to March 2017, our dataset includes all four seasons. Therefore, it contains various changes of weather, illumination, vegetation, and road construction. As shown in Fig. 6, we can see image changes at the same location as the season changes.</p>

                <div class="imgs">
                    <div class="half">
                        <img src="imgs/time_distribution.jpg" alt="time_distribution">
                        <figcaption>Fig. 5</figcaption>
                    </div>
                    <div class="half">
                        <img src="imgs/different_time.jpg" alt="different_time">
                        <figcaption>Fig. 6</figcaption>
                    </div>
                </div>

            </div>
        </section>

        <section class="main_section even_section" id="desp">
            <div class="content">
                <h1>Detail Description</h1>
                <hr class="horizontalLine">
                
                <div class="services-tabs">
                    <ul>
                        <li><a href="#desp1">Difficulty Level</a></li>
                        <li><a href="#desp2">Uniqueness</a></li>
                        <li><a href="#desp3">Front-View VS Side-View</a></li>
                        <li><a href="#desp4">Other Challenges</a></li>
                    </ul>
                    <br>

                    <div id="desp1" class="despTab">
                        <div class="despContainer">
                            <div>
                                <div class="tabBg">
                                    <h2>01</h2>
                                    <h3>Difficulty Level</h3>
                                    <p>
                                        We assign each side-view query image a difficulty level of easy, medium, or hard. First, we extract SIFT features for each image. Then for each query image, we find the top-8 closest side-view training images by GPS coordinates. The query image and its top-8 closest images form eight image pairs. We use RANSAC to compute a fundamental matrix and the number of inliers for each pair of images. We use three intervals to measure the difficulty level of matching each pair based on each pair's number of inliers points: 0-19 (hard), 20-80 (medium), >80 (easy). The interval values are determined by artificially viewing the image pairs and checking the similarity of the image pairs. The difficulty level of each side-view query image is the most common difficulty level of its eight pairs.
                                    </p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div id="desp2" class="despTab">
                        <div class="despContainer">
                            <div>
                                <div class="tabBg">
                                    <h2>02</h2>
                                    <h3>Uniqueness</h3>
                                    <p>
                                        Our dataset is unique in two ways. First, comparing to front-view images where sky and road surfaces occupy large areas, side-view images focus on street views such as shop signs and metro entrances. Second, we include image anonymization to protect the privacy of pedestrians and cars. In the meantime, anonymized images provide VPR algorithms static and environment-only information, getting rid of moving objects and pedestrians.
                                    </p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div id="desp3" class="despTab">
                        <div class="despContainer">
                            <div>
                                <div class="tabBg">
                                    <h2>03</h2>
                                    <h3>Front-View VS Side-View</h3>
                                    <p>
                                        Our dataset includes images in two view directions: front-view and side-view. Front-view images has a view direction that is parallel to the driving/street direction. Front-view images usually have features of roads, shapes of skylines, and textures of the roadside buildings. Contrarily, side-view images has a view direction facing buildings along street. Side view direction is perpendicular to front view direction.
                                    </p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div id="desp4" class="despTab">
                        <div class="despContainer">
                            <div>
                                <div class="tabBg">
                                    <h2>04</h2>
                                    <h3>Other Challenges</h3>
                                    <p>
                                        Because our dataset is one-year long, the images taken at the same location have artificial or natural differences. First, Fig. 7 left was taken in October 2016 with sideway constructions and the right was taken in December 2016 after the construction. At some locations, the construction may cover the whole image. Second, different seasons cause different appearances at the same location. Fig. 8 left was taken in summer, July 2016, and the right was taken in winter, January 2017. In this case, the vegetation in Washington Square Park had changed a lot and snow was covering the ground in winter. Furthermore, if the vehicle was moving fast, the images taken by the vehicle will be blurry (Fig. 9). Although two images were taken at the same location, the blurry one will cause more difficulty during VPR.
                                    </p>
                                    <div class="imgContainer">
                                        <img src="imgs/construction.jpg" alt="construction">
                                        <figcaption>Fig. 7 During and After Construction</figcaption>
    
                                        <img src="imgs/park.jpg" alt="season">
                                        <figcaption>Fig. 8 Summer and Winter</figcaption>
    
                                        <img src="imgs/blur.jpg" alt="blur">
                                        <figcaption>Fig. 9 Without and With Motion Blur</figcaption>
                                    </div>
                                    
                                </div>
                            </div>
                        </div>
                    </div>

                </div>
            </div>
        </section>

        <section class="main_section" id="download">
            <div class="content">
                <h1>Download</h1>
                <hr class="horizontalLine">
                <p>Files are organized as four zips and a .csv file. Zip files contain images and the csv file contains information for each corresponding image such as GPS coordinates and image-taken date and time. Please fill a Google form through the following link to request dataset access.</p>
                <h3>Google Form Link: <a href="https://docs.google.com/forms/d/e/1FAIpQLScCHtnsHTcq5GfqpTPlLdJMAHXIv3SP83rkTk5IdHGKzqE6mQ/viewform?usp=pp_url" target="_blank">Please Click Here</a></h3>
                <p>Below is the Github repo of code used to perform experiments described in the paper.</p>
                <h3>Code Link: <a href="https://github.com/ai4ce/NYU-VPR" target="_blank">Please Click Here</a></h3>
            </div>
        </section>

        <section class="main_section" id="contact">
            <div class="content">
                <h1>Contact Us</h1>
                <hr class="horizontalLine">
                <h3>Chen Feng - <a class="mail" href="mailto:cfeng@nyu.edu">cfeng@nyu.edu</a></h3>
                <h3>Diwei Sheng - <a class="mail" href="mailto:ds5725@nyu.edu">ds5725@nyu.edu</a></h3>
                <h3>Yuxiang Chai - <a class="mail" href="mailto:yc3743@nyu.edu">yc3743@nyu.edu</a></h3>
                <h3>Xinru Li - <a class="mail" href="mailto:xl2641@nyu.edu">xl2641@nyu.edu</a></h3>
            </div>
        </section>
        
        <section class="main_section" id="acknowledge">
            <div class="content">
                <h1>Acknowledgement</h1>
                <p>The raw data from Carmera was obtained from <a href="https://vida.engineering.nyu.edu/" target="_blank">NYU VIDA lab</a> led by Professor Claudio Silva. The project was funded by <a href="https://c2smart.engineering.nyu.edu/" target="_blank">C2SMART</a>.</p>
                <hr class="horizontalLine">
                <div class="imgs">
                    <div class="half">
                        <img src="imgs/vida_logo.png" alt="vida_logo">
                        <figcaption>VIDA Logo</figcaption>
                    </div>
                    <div class="half">
                        <img src="imgs/c2smart_logo.png" alt="c2smart_logo">
                        <figcaption>C2SMART Logo</figcaption>
                    </div>
                </div>
            </div>
        </section>
    </body>
</html>
